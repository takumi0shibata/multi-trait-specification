{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_asap_dataset, load_toefl_dataset, get_score_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset # Rename to avoid conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"ASAP\"\n",
    "# TASK = \"TOEFL11\"\n",
    "\n",
    "PROMPT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"ASAP\":\n",
    "    df_test = load_asap_dataset('datasets/ASAP', stratify=True)\n",
    "    df_train = load_asap_dataset('datasets/ASAP', stratify=False).filter(~pl.col(\"essay_id\").is_in(df_test['essay_id']))\n",
    "elif TASK == \"TOEFL11\":\n",
    "    df = load_toefl_dataset('datasets/TOEFL11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.filter(pl.col(\"essay_set\") == PROMPT)\n",
    "df_val = df_val.filter(pl.col(\"essay_set\") == PROMPT)\n",
    "df_test = df_test.filter(pl.col(\"essay_set\") == PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score, max_score = get_score_range(TASK, PROMPT)\n",
    "df_train = df_train.with_columns(\n",
    "    ((pl.col(\"score\") - min_score) / (max_score - min_score)).alias(\"normalized_score\")\n",
    ")\n",
    "df_val = df_val.with_columns(\n",
    "    ((pl.col(\"score\") - min_score) / (max_score - min_score)).alias(\"normalized_score\")\n",
    ")\n",
    "df_test = df_test.with_columns(\n",
    "    ((pl.col(\"score\") - min_score) / (max_score - min_score)).alias(\"normalized_score\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_276, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_set</th><th>essay_id</th><th>essay</th><th>score</th><th>normalized_score</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>1628</td><td>&quot;Dear local newspaper, I think …</td><td>10</td><td>0.8</td></tr><tr><td>1</td><td>562</td><td>&quot;Dear @CAPS1 @CAPS2 @CAPS3, @CA…</td><td>10</td><td>0.8</td></tr><tr><td>1</td><td>1746</td><td>&quot;Computers can take a lot of a …</td><td>10</td><td>0.8</td></tr><tr><td>1</td><td>1430</td><td>&quot;Dear Local Newspaper, @CAPS1 y…</td><td>8</td><td>0.6</td></tr><tr><td>1</td><td>441</td><td>&quot;Dear Local Newspaper: Computer…</td><td>8</td><td>0.6</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1</td><td>1107</td><td>&quot;Computer are good and bad beca…</td><td>4</td><td>0.2</td></tr><tr><td>1</td><td>1300</td><td>&quot;I do not beileve that computer…</td><td>7</td><td>0.5</td></tr><tr><td>1</td><td>245</td><td>&quot;I believe computers are a bene…</td><td>7</td><td>0.5</td></tr><tr><td>1</td><td>1266</td><td>&quot;Guess what! Do you like to use…</td><td>8</td><td>0.6</td></tr><tr><td>1</td><td>323</td><td>&quot;Dear @CAPS1, I feel as though …</td><td>10</td><td>0.8</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_276, 5)\n",
       "┌───────────┬──────────┬─────────────────────────────────┬───────┬──────────────────┐\n",
       "│ essay_set ┆ essay_id ┆ essay                           ┆ score ┆ normalized_score │\n",
       "│ ---       ┆ ---      ┆ ---                             ┆ ---   ┆ ---              │\n",
       "│ i64       ┆ i64      ┆ str                             ┆ i64   ┆ f64              │\n",
       "╞═══════════╪══════════╪═════════════════════════════════╪═══════╪══════════════════╡\n",
       "│ 1         ┆ 1628     ┆ Dear local newspaper, I think … ┆ 10    ┆ 0.8              │\n",
       "│ 1         ┆ 562      ┆ Dear @CAPS1 @CAPS2 @CAPS3, @CA… ┆ 10    ┆ 0.8              │\n",
       "│ 1         ┆ 1746     ┆ Computers can take a lot of a … ┆ 10    ┆ 0.8              │\n",
       "│ 1         ┆ 1430     ┆ Dear Local Newspaper, @CAPS1 y… ┆ 8     ┆ 0.6              │\n",
       "│ 1         ┆ 441      ┆ Dear Local Newspaper: Computer… ┆ 8     ┆ 0.6              │\n",
       "│ …         ┆ …        ┆ …                               ┆ …     ┆ …                │\n",
       "│ 1         ┆ 1107     ┆ Computer are good and bad beca… ┆ 4     ┆ 0.2              │\n",
       "│ 1         ┆ 1300     ┆ I do not beileve that computer… ┆ 7     ┆ 0.5              │\n",
       "│ 1         ┆ 245      ┆ I believe computers are a bene… ┆ 7     ┆ 0.5              │\n",
       "│ 1         ┆ 1266     ┆ Guess what! Do you like to use… ┆ 8     ┆ 0.6              │\n",
       "│ 1         ┆ 323      ┆ Dear @CAPS1, I feel as though … ┆ 10    ┆ 0.8              │\n",
       "└───────────┴──────────┴─────────────────────────────────┴───────┴──────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18281/465151571.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning for bert-base-uncased...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 01:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Qwk</th>\n",
       "      <th>Lwk</th>\n",
       "      <th>Corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.122609</td>\n",
       "      <td>0.099040</td>\n",
       "      <td>0.653756</td>\n",
       "      <td>0.423783</td>\n",
       "      <td>0.804534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.089262</td>\n",
       "      <td>0.068933</td>\n",
       "      <td>0.827873</td>\n",
       "      <td>0.620772</td>\n",
       "      <td>0.841496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>0.086216</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>0.807460</td>\n",
       "      <td>0.609521</td>\n",
       "      <td>0.854762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>0.085721</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.823129</td>\n",
       "      <td>0.616575</td>\n",
       "      <td>0.850087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.007672</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>0.067754</td>\n",
       "      <td>0.812728</td>\n",
       "      <td>0.599584</td>\n",
       "      <td>0.847692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>0.087557</td>\n",
       "      <td>0.067467</td>\n",
       "      <td>0.809830</td>\n",
       "      <td>0.606230</td>\n",
       "      <td>0.844440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.096649</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.784794</td>\n",
       "      <td>0.559370</td>\n",
       "      <td>0.841634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.092097</td>\n",
       "      <td>0.072645</td>\n",
       "      <td>0.791346</td>\n",
       "      <td>0.566434</td>\n",
       "      <td>0.839802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.091420</td>\n",
       "      <td>0.071898</td>\n",
       "      <td>0.800701</td>\n",
       "      <td>0.572546</td>\n",
       "      <td>0.838835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.090719</td>\n",
       "      <td>0.071317</td>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.576075</td>\n",
       "      <td>0.837811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed.\n",
      "Evaluating the best model on the validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.00664439145475626, 'eval_rmse': 0.08151313662528992, 'eval_mae': 0.0624455027282238, 'eval_qwk': 0.851268068576551, 'eval_lwk': 0.6590455787840732, 'eval_corr': 0.8718656008644179, 'eval_runtime': 0.4587, 'eval_samples_per_second': 394.557, 'eval_steps_per_second': 13.079, 'epoch': 10.0}\n",
      "Saving metrics...\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Specify the pre-trained model name. Can be changed to \"bert-base-uncased\", \"FacebookAI/roberta-base\", \"microsoft/deberta-v3-large\", etc.\n",
    "model_name = \"bert-base-uncased\"\n",
    "num_train_epochs = 10 # Number of training epochs (adjust as needed)\n",
    "batch_size = 32 # Batch size per device (adjust based on GPU memory)\n",
    "max_length = 512 # Max sequence length for tokenizer\n",
    "\n",
    "# --- 3. Load Tokenizer and Model ---\n",
    "# Load the tokenizer associated with the chosen pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the pre-trained model for sequence classification.\n",
    "# Set num_labels=1 for regression tasks.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# --- 4. Tokenize Data ---\n",
    "# Tokenize the texts using the loaded tokenizer\n",
    "train_encodings = tokenizer(df_train['essay'].to_list(), truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "dev_encodings = tokenizer(df_val['essay'].to_list(), truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "test_encodings = tokenizer(df_test['essay'].to_list(), truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "# --- 5. Create Custom PyTorch Dataset ---\n",
    "class EssayDatasetTmp(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve tokenized inputs for the given index\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add the corresponding label, converting it to a tensor\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float) # Ensure label is float tensor\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.labels)\n",
    "\n",
    "# Instantiate the custom dataset for training and evaluation sets\n",
    "train_dataset = EssayDatasetTmp(train_encodings, df_train['normalized_score'])\n",
    "dev_dataset = EssayDatasetTmp(dev_encodings, df_val['normalized_score'])\n",
    "test_dataset = EssayDatasetTmp(test_encodings, df_test['normalized_score'])\n",
    "\n",
    "# --- 6. Define Training Arguments ---\n",
    "# Configure the training process using TrainingArguments (remains the same)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs/prompt-specific',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1, # Number of steps for learning rate warmup\n",
    "    weight_decay=2e-5, # Strength of weight decay regularization\n",
    "    optim=\"adamw_torch\", # Use the AdamW optimizer\n",
    "    logging_strategy=\"steps\",  # Log metrics at the end of each epoch\n",
    "    logging_steps=10, # Log every 10 steps\n",
    "    eval_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False, # Load the best model found during training at the end\n",
    "    metric_for_best_model=\"eval_qwk\", # Use Mean Squared Error to determine the best model\n",
    "    greater_is_better=True, # Lower MSE is better\n",
    "    report_to=\"none\", # Disable external reporting integrations like WandB/TensorBoard for simplicity\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision training if a GPU is available\n",
    ")\n",
    "\n",
    "# --- 7. Define Compute Metrics Function ---\n",
    "# Define a function to compute metrics during evaluation (MSE, MAE, and QWK for regression)\n",
    "def prepare_compute_metrics(minscore, maxscore):\n",
    "    def compute_metrics(eval_pred: EvalPrediction):\n",
    "        predictions, labels = eval_pred\n",
    "        # Predictions might be logits or regression outputs, squeeze them if necessary\n",
    "        if len(predictions.shape) > 1:\n",
    "            predictions = predictions.squeeze(-1)\n",
    "\n",
    "        # Calculate standard regression metrics\n",
    "        rmse = np.sqrt(mean_squared_error(labels, predictions))\n",
    "        mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "        # Convert predictions and labels to scores based on min/max values\n",
    "        # This step is necessary for calculating QWK\n",
    "        predictions = predictions * (maxscore - minscore) + minscore\n",
    "        labels = labels * (maxscore - minscore) + minscore\n",
    "        qwk = cohen_kappa_score(np.round(predictions), np.round(labels), weights=\"quadratic\", labels=[i for i in range(minscore, maxscore + 1)])\n",
    "        lwk = cohen_kappa_score(np.round(predictions), np.round(labels), weights=\"linear\", labels=[i for i in range(minscore, maxscore + 1)])\n",
    "\n",
    "        # Calculate Correlation Coefficient\n",
    "        corr = np.corrcoef(predictions, labels)[0, 1]\n",
    "\n",
    "        return {\"rmse\": rmse, \"mae\": mae, \"qwk\": qwk, \"lwk\": lwk, \"corr\": corr}\n",
    "    return compute_metrics\n",
    "\n",
    "# --- 8. Instantiate Trainer ---\n",
    "# Initialize the Trainer with the model, arguments, custom datasets, tokenizer, and metrics function\n",
    "# Note: The tokenizer is still passed for potential use cases like saving, but not strictly needed for data loading now.\n",
    "min_score, max_score = get_score_range(TASK, PROMPT)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=prepare_compute_metrics(min_score, max_score),\n",
    ")\n",
    "\n",
    "# --- 9. Train the Model ---\n",
    "print(f\"Starting fine-tuning for {model_name}...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning completed.\")\n",
    "\n",
    "# --- (Optional) Evaluate the Best Model ---\n",
    "print(\"Evaluating the best model on the validation set...\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Save Metrics with json\n",
    "print(\"Saving metrics...\")\n",
    "with open(f\"outputs/prompt-specific/prompt{PROMPT}.json\", \"w\") as metrics_file:\n",
    "    json.dump(eval_results, metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
